{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "practice_reinforce_pytorch.ipynb",
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBIDgRflDI_o"
      },
      "source": [
        "# REINFORCE in PyTorch\n",
        "\n",
        "Just like we did before for Q-learning, this time we'll design a PyTorch network to learn `CartPole-v0` via policy gradient (REINFORCE).\n",
        "\n",
        "Most of the code in this notebook is taken from approximate Q-learning, so you'll find it more or less familiar and even simpler."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2UXrmcoDI_q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16dbccac-49de-4219-bfb2-a0372c8786a3"
      },
      "source": [
        "import sys, os\n",
        "if 'google.colab' in sys.modules and not os.path.exists('.setup_complete'):\n",
        "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/setup_colab.sh -O- | bash\n",
        "\n",
        "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/coursera/grading.py -O ../grading.py\n",
        "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/coursera/week5_policy_based/submit.py\n",
        "\n",
        "    !touch .setup_complete\n",
        "\n",
        "# This code creates a virtual display to draw game images on.\n",
        "# It will have no effect if your machine has a monitor.\n",
        "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
        "    !bash ../xvfb start\n",
        "    os.environ['DISPLAY'] = ':1'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting virtual X frame buffer: Xvfb.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxlCSPuXDI_r"
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIpLxnEDDI_r"
      },
      "source": [
        "A caveat: with some versions of `pyglet`, the following cell may crash with `NameError: name 'base' is not defined`. The corresponding bug report is [here](https://github.com/pyglet/pyglet/issues/134). If you see this error, try restarting the kernel."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5czNLyXEDI_r",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "outputId": "5efb95e8-40df-4f3c-e517-70340c84c8e9"
      },
      "source": [
        "env = gym.make(\"CartPole-v0\")\n",
        "\n",
        "# gym compatibility: unwrap TimeLimit\n",
        "if hasattr(env, '_max_episode_steps'):\n",
        "    env = env.env\n",
        "\n",
        "env.reset()\n",
        "n_actions = env.action_space.n\n",
        "state_dim = env.observation_space.shape\n",
        "\n",
        "plt.imshow(env.render(\"rgb_array\"))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f7f3576c550>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hB6LQTgDI_s"
      },
      "source": [
        "# Building the network for REINFORCE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HcQj_To7DI_s"
      },
      "source": [
        "For REINFORCE algorithm, we'll need a model that predicts action probabilities given states.\n",
        "\n",
        "For numerical stability, please __do not include the softmax layer into your network architecture__.\n",
        "We'll use softmax or log-softmax where appropriate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lF6fnAjdDI_s"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M73CYZNiDI_t"
      },
      "source": [
        "# Build a simple neural network that predicts policy logits. \n",
        "# Keep it simple: CartPole isn't worth deep architectures.\n",
        "model = nn.Sequential(\n",
        "nn.Linear(state_dim[0],256),\n",
        "nn.ReLU(),\n",
        "nn.Linear(256,128),\n",
        "nn.ReLU(),\n",
        "nn.Linear(128,n_actions)\n",
        ")\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWpiVMwoDI_t"
      },
      "source": [
        "#### Predict function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "neFCtLToDI_t"
      },
      "source": [
        "Note: output value of this function is not a torch tensor, it's a numpy array.\n",
        "So, here gradient calculation is not needed.\n",
        "<br>\n",
        "Use [no_grad](https://pytorch.org/docs/stable/autograd.html#torch.autograd.no_grad)\n",
        "to suppress gradient calculation.\n",
        "<br>\n",
        "Also, `.detach()` (or legacy `.data` property) can be used instead, but there is a difference:\n",
        "<br>\n",
        "With `.detach()` computational graph is built but then disconnected from a particular tensor,\n",
        "so `.detach()` should be used if that graph is needed for backprop via some other (not detached) tensor;\n",
        "<br>\n",
        "In contrast, no graph is built by any operation in `no_grad()` context, thus it's preferable here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45py1xgTDI_u"
      },
      "source": [
        "def predict_probs(states):\n",
        "    \"\"\" \n",
        "    Predict action probabilities given states.\n",
        "    :param states: numpy array of shape [batch, state_shape]\n",
        "    :returns: numpy array of shape [batch, n_actions]\n",
        "    \"\"\"\n",
        "    # convert states, compute (logits, use softmax to get probability\n",
        "    \n",
        "    out=model(torch.tensor(states,dtype=torch.float32))\n",
        "    with torch.no_grad():\n",
        "      out=nn.Softmax(dim=1)(out)\n",
        "    return out.numpy()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YbalB-VfDI_u"
      },
      "source": [
        "test_states = np.array([env.reset() for _ in range(5)])\n",
        "test_probas = predict_probs(test_states)\n",
        "\n",
        "assert isinstance(test_probas, np.ndarray), \\\n",
        "    \"you must return np array and not %s\" % type(test_probas)\n",
        "assert tuple(test_probas.shape) == (test_states.shape[0], env.action_space.n), \\\n",
        "    \"wrong output shape: %s\" % np.shape(test_probas)\n",
        "assert np.allclose(np.sum(test_probas, axis=1), 1), \"probabilities do not sum to 1\""
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5B7SbIhfDI_u"
      },
      "source": [
        "### Play the game\n",
        "\n",
        "We can now use our newly built agent to play the game."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IwPfe3ZoDI_v"
      },
      "source": [
        "def generate_session(env, t_max=1000):\n",
        "    \"\"\" \n",
        "    Play a full session with REINFORCE agent.\n",
        "    Returns sequences of states, actions, and rewards.\n",
        "    \"\"\"\n",
        "    # arrays to record session\n",
        "    states, actions, rewards = [], [], []\n",
        "    s = env.reset()\n",
        "\n",
        "    for t in range(t_max):\n",
        "        # action probabilities array aka pi(a|s)\n",
        "        action_probs = predict_probs(np.array([s]))[0]\n",
        "\n",
        "        # Sample action with given probabilities.\n",
        "        a = np.random.choice([0,1],p=action_probs)\n",
        "        new_s, r, done, info = env.step(a)\n",
        "\n",
        "        # record session history to train later\n",
        "        states.append(s)\n",
        "        actions.append(a)\n",
        "        rewards.append(r)\n",
        "\n",
        "        s = new_s\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    return states, actions, rewards"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwSGHkXEDI_v"
      },
      "source": [
        "# test it\n",
        "states, actions, rewards = generate_session(env)"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dM89ZAkDI_w"
      },
      "source": [
        "### Computing cumulative rewards\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "G_t &= r_t + \\gamma r_{t + 1} + \\gamma^2 r_{t + 2} + \\ldots \\\\\n",
        "&= \\sum_{i = t}^T \\gamma^{i - t} r_i \\\\\n",
        "&= r_t + \\gamma * G_{t + 1}\n",
        "\\end{align*}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wCYFFapDI_w"
      },
      "source": [
        "def get_cumulative_rewards(rewards,  # rewards at each step\n",
        "                           gamma=0.99  # discount for reward\n",
        "                           ):\n",
        "    \"\"\"\n",
        "    Take a list of immediate rewards r(s,a) for the whole session \n",
        "    and compute cumulative returns (a.k.a. G(s,a) in Sutton '16).\n",
        "    \n",
        "    G_t = r_t + gamma*r_{t+1} + gamma^2*r_{t+2} + ...\n",
        "\n",
        "    A simple way to compute cumulative rewards is to iterate from the last\n",
        "    to the first timestep and compute G_t = r_t + gamma*G_{t+1} recurrently\n",
        "\n",
        "    You must return an array/list of cumulative rewards with as many elements as in the initial rewards.\n",
        "    \"\"\"\n",
        "    \n",
        "    cumulative=[]\n",
        "    for t in range(len(rewards)):      \n",
        "      sum=0\n",
        "      i=0\n",
        "      for j in range(len(rewards)):\n",
        "        if t+j<=len(rewards)-1:\n",
        "          sum+=(gamma**(i))*rewards[t+j]\n",
        "          i+=1\n",
        "      \n",
        "      cumulative.append(sum)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    return cumulative"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHKQGUTHDI_x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2909825a-298b-4cf3-9c69-b12d97dc0231"
      },
      "source": [
        "get_cumulative_rewards(rewards)\n",
        "assert len(get_cumulative_rewards(list(range(100)))) == 100\n",
        "assert np.allclose(\n",
        "    get_cumulative_rewards([0, 0, 1, 0, 0, 1, 0], gamma=0.9),\n",
        "    [1.40049, 1.5561, 1.729, 0.81, 0.9, 1.0, 0.0])\n",
        "assert np.allclose(\n",
        "    get_cumulative_rewards([0, 0, 1, -2, 3, -4, 0], gamma=0.5),\n",
        "    [0.0625, 0.125, 0.25, -1.5, 1.0, -4.0, 0.0])\n",
        "assert np.allclose(\n",
        "    get_cumulative_rewards([0, 0, 1, 2, 3, 4, 0], gamma=0),\n",
        "    [0, 0, 1, 2, 3, 4, 0])\n",
        "print(\"looks good!\")"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "looks good!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OaixhhOdDI_x"
      },
      "source": [
        "#### Loss function and updates\n",
        "\n",
        "We now need to define objective and update over policy gradient.\n",
        "\n",
        "Our objective function is\n",
        "\n",
        "$$ J \\approx  { 1 \\over N } \\sum_{s_i,a_i} G(s_i,a_i) $$\n",
        "\n",
        "REINFORCE defines a way to compute the gradient of the expected reward with respect to policy parameters. The formula is as follows:\n",
        "\n",
        "$$ \\nabla_\\theta \\hat J(\\theta) \\approx { 1 \\over N } \\sum_{s_i, a_i} \\nabla_\\theta \\log \\pi_\\theta (a_i \\mid s_i) \\cdot G_t(s_i, a_i) $$\n",
        "\n",
        "We can abuse PyTorch's capabilities for automatic differentiation by defining our objective function as follows:\n",
        "\n",
        "$$ \\hat J(\\theta) \\approx { 1 \\over N } \\sum_{s_i, a_i} \\log \\pi_\\theta (a_i \\mid s_i) \\cdot G_t(s_i, a_i) $$\n",
        "\n",
        "When you compute the gradient of that function with respect to network weights $\\theta$, it will become exactly the policy gradient."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ATix7B3_DI_y"
      },
      "source": [
        "def to_one_hot(y_tensor, ndims):\n",
        "    \"\"\" helper: take an integer vector and convert it to 1-hot matrix. \"\"\"\n",
        "    y_tensor = y_tensor.type(torch.LongTensor).view(-1, 1)\n",
        "    y_one_hot = torch.zeros(\n",
        "        y_tensor.size()[0], ndims).scatter_(1, y_tensor, 1)\n",
        "    return y_one_hot"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ZH4AFeIDI_y"
      },
      "source": [
        "# Your code: define optimizers\n",
        "optimizer = torch.optim.Adam(model.parameters(), 1e-3)\n",
        "\n",
        "\n",
        "def train_on_session(states, actions, rewards, gamma=0.99, entropy_coef=1e-2):\n",
        "    \"\"\"\n",
        "    Takes a sequence of states, actions and rewards produced by generate_session.\n",
        "    Updates agent's weights by following the policy gradient above.\n",
        "    Please use Adam optimizer with default parameters.\n",
        "    \"\"\"\n",
        "\n",
        "    # cast everything into torch tensors\n",
        "    states = torch.tensor(states, dtype=torch.float32)\n",
        "    actions = torch.tensor(actions, dtype=torch.int32)\n",
        "    cumulative_returns = np.array(get_cumulative_rewards(rewards, gamma))\n",
        "    cumulative_returns = torch.tensor(cumulative_returns, dtype=torch.float32)\n",
        "\n",
        "    # predict logits, probas and log-probas using an agent.\n",
        "    logits = model(states)\n",
        "    probs = nn.functional.softmax(logits, -1)\n",
        "    log_probs = nn.functional.log_softmax(logits, -1)\n",
        "\n",
        "    assert all(isinstance(v, torch.Tensor) for v in [logits, probs, log_probs]), \\\n",
        "        \"please use compute using torch tensors and don't use predict_probs function\"\n",
        "\n",
        "    # select log-probabilities for chosen actions, log pi(a_i|s_i)\n",
        "    log_probs_for_actions = torch.sum(\n",
        "        log_probs * to_one_hot(actions, env.action_space.n), dim=1)\n",
        "   \n",
        "    # Compute loss here. Don't forgen entropy regularization with `entropy_coef` \n",
        "    \n",
        "    entropy = torch.sum(probs*log_probs)\n",
        "    \n",
        "    \n",
        "    loss =-torch.add(torch.mean(log_probs_for_actions * cumulative_returns),torch.mul(entropy,entropy_coef)) \n",
        "\n",
        "    # Gradient descent step\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # technical: return session rewards to print them later\n",
        "    return np.sum(rewards)"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3PI-OlDDI_z"
      },
      "source": [
        "### The actual training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnqs7m8wDI_z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbf116e4-7ef7-4f0e-995d-c73106ecfefa"
      },
      "source": [
        "for i in range(100):\n",
        "    rewards = [train_on_session(*generate_session(env)) for _ in range(100)]  # generate new sessions\n",
        "    \n",
        "    print(\"mean reward:%.3f\" % (np.mean(rewards)))\n",
        "    \n",
        "    if np.mean(rewards) > 300:\n",
        "        print(\"You Win!\")  # but you can train even further\n",
        "        break"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mean reward:144.140\n",
            "mean reward:200.770\n",
            "mean reward:737.050\n",
            "You Win!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVDy06qADI_z"
      },
      "source": [
        "### Results & video"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EEHqa69TDI_z"
      },
      "source": [
        "# Record sessions\n",
        "\n",
        "import gym.wrappers\n",
        "\n",
        "with gym.wrappers.Monitor(gym.make(\"CartPole-v0\"), directory=\"videos\", force=True) as env_monitor:\n",
        "    sessions = [generate_session(env_monitor) for _ in range(100)]"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjEzk_cqDI_0"
      },
      "source": [
        "# Show video. This may not work in some setups. If it doesn't\n",
        "# work for you, you can download the videos and view them locally.\n",
        "\n",
        "from pathlib import Path\n",
        "from IPython.display import HTML\n",
        "\n",
        "video_names = sorted([s for s in Path('videos').iterdir() if s.suffix == '.mp4'])\n",
        "\n",
        "HTML(\"\"\"\n",
        "<video width=\"640\" height=\"480\" controls>\n",
        "  <source src=\"{}\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\".format(video_names[-1]))  # You can also try other indices"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVt9c2qjDI_0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27d4a61a-7595-4a95-b535-15b3fc32579c"
      },
      "source": [
        "from submit import submit_cartpole\n",
        "submit_cartpole(generate_session, '', '')"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Your average reward is 904.05 over 100 episodes\n",
            "Submitted to Coursera platform. See results on assignment page!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRww4DSnDI_0"
      },
      "source": [
        "That's all, thank you for your attention!\n",
        "\n",
        "Not having enough? There's an actor-critic waiting for you in the honor section. But make sure you've seen the videos first."
      ]
    }
  ]
}