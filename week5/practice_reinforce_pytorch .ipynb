{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "practice_reinforce_pytorch.ipynb",
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBIDgRflDI_o"
      },
      "source": [
        "# REINFORCE in PyTorch\n",
        "\n",
        "Just like we did before for Q-learning, this time we'll design a PyTorch network to learn `CartPole-v0` via policy gradient (REINFORCE).\n",
        "\n",
        "Most of the code in this notebook is taken from approximate Q-learning, so you'll find it more or less familiar and even simpler."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2UXrmcoDI_q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b819e4d4-db5a-4052-d1b5-9bfbf7d064c4"
      },
      "source": [
        "import sys, os\n",
        "if 'google.colab' in sys.modules and not os.path.exists('.setup_complete'):\n",
        "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/setup_colab.sh -O- | bash\n",
        "\n",
        "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/coursera/grading.py -O ../grading.py\n",
        "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/coursera/week5_policy_based/submit.py\n",
        "\n",
        "    !touch .setup_complete\n",
        "\n",
        "# This code creates a virtual display to draw game images on.\n",
        "# It will have no effect if your machine has a monitor.\n",
        "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
        "    !bash ../xvfb start\n",
        "    os.environ['DISPLAY'] = ':1'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Selecting previously unselected package xvfb.\n",
            "(Reading database ... 160837 files and directories currently installed.)\n",
            "Preparing to unpack .../xvfb_2%3a1.19.6-1ubuntu4.9_amd64.deb ...\n",
            "Unpacking xvfb (2:1.19.6-1ubuntu4.9) ...\n",
            "Setting up xvfb (2:1.19.6-1ubuntu4.9) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Starting virtual X frame buffer: Xvfb.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxlCSPuXDI_r"
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIpLxnEDDI_r"
      },
      "source": [
        "A caveat: with some versions of `pyglet`, the following cell may crash with `NameError: name 'base' is not defined`. The corresponding bug report is [here](https://github.com/pyglet/pyglet/issues/134). If you see this error, try restarting the kernel."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5czNLyXEDI_r",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "outputId": "31996040-66b2-4adf-87d0-e3b926e3d784"
      },
      "source": [
        "env = gym.make(\"CartPole-v0\")\n",
        "\n",
        "# gym compatibility: unwrap TimeLimit\n",
        "if hasattr(env, '_max_episode_steps'):\n",
        "    env = env.env\n",
        "\n",
        "env.reset()\n",
        "n_actions = env.action_space.n\n",
        "state_dim = env.observation_space.shape\n",
        "\n",
        "plt.imshow(env.render(\"rgb_array\"))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f11dc2e3350>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hB6LQTgDI_s"
      },
      "source": [
        "# Building the network for REINFORCE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HcQj_To7DI_s"
      },
      "source": [
        "For REINFORCE algorithm, we'll need a model that predicts action probabilities given states.\n",
        "\n",
        "For numerical stability, please __do not include the softmax layer into your network architecture__.\n",
        "We'll use softmax or log-softmax where appropriate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lF6fnAjdDI_s"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M73CYZNiDI_t"
      },
      "source": [
        "# Build a simple neural network that predicts policy logits. \n",
        "# Keep it simple: CartPole isn't worth deep architectures.\n",
        "model = nn.Sequential(\n",
        "nn.Linear(state_dim[0],256),\n",
        "nn.ReLU(),\n",
        "nn.Linear(256,128),\n",
        "nn.ReLU(),\n",
        "nn.Linear(128,n_actions)\n",
        ")\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWpiVMwoDI_t"
      },
      "source": [
        "#### Predict function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "neFCtLToDI_t"
      },
      "source": [
        "Note: output value of this function is not a torch tensor, it's a numpy array.\n",
        "So, here gradient calculation is not needed.\n",
        "<br>\n",
        "Use [no_grad](https://pytorch.org/docs/stable/autograd.html#torch.autograd.no_grad)\n",
        "to suppress gradient calculation.\n",
        "<br>\n",
        "Also, `.detach()` (or legacy `.data` property) can be used instead, but there is a difference:\n",
        "<br>\n",
        "With `.detach()` computational graph is built but then disconnected from a particular tensor,\n",
        "so `.detach()` should be used if that graph is needed for backprop via some other (not detached) tensor;\n",
        "<br>\n",
        "In contrast, no graph is built by any operation in `no_grad()` context, thus it's preferable here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45py1xgTDI_u"
      },
      "source": [
        "def predict_probs(states):\n",
        "    \"\"\" \n",
        "    Predict action probabilities given states.\n",
        "    :param states: numpy array of shape [batch, state_shape]\n",
        "    :returns: numpy array of shape [batch, n_actions]\n",
        "    \"\"\"\n",
        "    # convert states, compute (logits, use softmax to get probability\n",
        "    \n",
        "    out=model(torch.tensor(states,dtype=torch.float32))\n",
        "    with torch.no_grad():\n",
        "      out=nn.Softmax(dim=1)(out)\n",
        "    return out.numpy()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YbalB-VfDI_u"
      },
      "source": [
        "test_states = np.array([env.reset() for _ in range(5)])\n",
        "test_probas = predict_probs(test_states)\n",
        "\n",
        "assert isinstance(test_probas, np.ndarray), \\\n",
        "    \"you must return np array and not %s\" % type(test_probas)\n",
        "assert tuple(test_probas.shape) == (test_states.shape[0], env.action_space.n), \\\n",
        "    \"wrong output shape: %s\" % np.shape(test_probas)\n",
        "assert np.allclose(np.sum(test_probas, axis=1), 1), \"probabilities do not sum to 1\""
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5B7SbIhfDI_u"
      },
      "source": [
        "### Play the game\n",
        "\n",
        "We can now use our newly built agent to play the game."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IwPfe3ZoDI_v"
      },
      "source": [
        "def generate_session(env, t_max=1000):\n",
        "    \"\"\" \n",
        "    Play a full session with REINFORCE agent.\n",
        "    Returns sequences of states, actions, and rewards.\n",
        "    \"\"\"\n",
        "    # arrays to record session\n",
        "    states, actions, rewards = [], [], []\n",
        "    s = env.reset()\n",
        "\n",
        "    for t in range(t_max):\n",
        "        # action probabilities array aka pi(a|s)\n",
        "        action_probs = predict_probs(np.array([s]))[0]\n",
        "\n",
        "        # Sample action with given probabilities.\n",
        "        a = np.argmax(action_probs)\n",
        "        new_s, r, done, info = env.step(a)\n",
        "\n",
        "        # record session history to train later\n",
        "        states.append(s)\n",
        "        actions.append(a)\n",
        "        rewards.append(r)\n",
        "\n",
        "        s = new_s\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    return states, actions, rewards"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwSGHkXEDI_v"
      },
      "source": [
        "# test it\n",
        "states, actions, rewards = generate_session(env)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dM89ZAkDI_w"
      },
      "source": [
        "### Computing cumulative rewards\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "G_t &= r_t + \\gamma r_{t + 1} + \\gamma^2 r_{t + 2} + \\ldots \\\\\n",
        "&= \\sum_{i = t}^T \\gamma^{i - t} r_i \\\\\n",
        "&= r_t + \\gamma * G_{t + 1}\n",
        "\\end{align*}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wCYFFapDI_w"
      },
      "source": [
        "def get_cumulative_rewards(rewards,  # rewards at each step\n",
        "                           gamma=0.99  # discount for reward\n",
        "                           ):\n",
        "    \"\"\"\n",
        "    Take a list of immediate rewards r(s,a) for the whole session \n",
        "    and compute cumulative returns (a.k.a. G(s,a) in Sutton '16).\n",
        "    \n",
        "    G_t = r_t + gamma*r_{t+1} + gamma^2*r_{t+2} + ...\n",
        "\n",
        "    A simple way to compute cumulative rewards is to iterate from the last\n",
        "    to the first timestep and compute G_t = r_t + gamma*G_{t+1} recurrently\n",
        "\n",
        "    You must return an array/list of cumulative rewards with as many elements as in the initial rewards.\n",
        "    \"\"\"\n",
        "    \n",
        "    cumulative=[]\n",
        "    for t in range(len(rewards)):      \n",
        "      sum=0\n",
        "      i=0\n",
        "      for j in range(len(rewards)):\n",
        "        if t+j<=len(rewards)-1:\n",
        "          sum+=(gamma**(i))*rewards[t+j]\n",
        "          i+=1\n",
        "      \n",
        "      cumulative.append(sum)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    return cumulative"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHKQGUTHDI_x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ec6be59-8157-402e-cdd8-99fa77158669"
      },
      "source": [
        "get_cumulative_rewards(rewards)\n",
        "assert len(get_cumulative_rewards(list(range(100)))) == 100\n",
        "assert np.allclose(\n",
        "    get_cumulative_rewards([0, 0, 1, 0, 0, 1, 0], gamma=0.9),\n",
        "    [1.40049, 1.5561, 1.729, 0.81, 0.9, 1.0, 0.0])\n",
        "assert np.allclose(\n",
        "    get_cumulative_rewards([0, 0, 1, -2, 3, -4, 0], gamma=0.5),\n",
        "    [0.0625, 0.125, 0.25, -1.5, 1.0, -4.0, 0.0])\n",
        "assert np.allclose(\n",
        "    get_cumulative_rewards([0, 0, 1, 2, 3, 4, 0], gamma=0),\n",
        "    [0, 0, 1, 2, 3, 4, 0])\n",
        "print(\"looks good!\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "looks good!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OaixhhOdDI_x"
      },
      "source": [
        "#### Loss function and updates\n",
        "\n",
        "We now need to define objective and update over policy gradient.\n",
        "\n",
        "Our objective function is\n",
        "\n",
        "$$ J \\approx  { 1 \\over N } \\sum_{s_i,a_i} G(s_i,a_i) $$\n",
        "\n",
        "REINFORCE defines a way to compute the gradient of the expected reward with respect to policy parameters. The formula is as follows:\n",
        "\n",
        "$$ \\nabla_\\theta \\hat J(\\theta) \\approx { 1 \\over N } \\sum_{s_i, a_i} \\nabla_\\theta \\log \\pi_\\theta (a_i \\mid s_i) \\cdot G_t(s_i, a_i) $$\n",
        "\n",
        "We can abuse PyTorch's capabilities for automatic differentiation by defining our objective function as follows:\n",
        "\n",
        "$$ \\hat J(\\theta) \\approx { 1 \\over N } \\sum_{s_i, a_i} \\log \\pi_\\theta (a_i \\mid s_i) \\cdot G_t(s_i, a_i) $$\n",
        "\n",
        "When you compute the gradient of that function with respect to network weights $\\theta$, it will become exactly the policy gradient."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ATix7B3_DI_y"
      },
      "source": [
        "def to_one_hot(y_tensor, ndims):\n",
        "    \"\"\" helper: take an integer vector and convert it to 1-hot matrix. \"\"\"\n",
        "    y_tensor = y_tensor.type(torch.LongTensor).view(-1, 1)\n",
        "    y_one_hot = torch.zeros(\n",
        "        y_tensor.size()[0], ndims).scatter_(1, y_tensor, 1)\n",
        "    return y_one_hot"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ZH4AFeIDI_y"
      },
      "source": [
        "# Your code: define optimizers\n",
        "optimizer = torch.optim.Adam(model.parameters(), 1e-3)\n",
        "\n",
        "\n",
        "def train_on_session(states, actions, rewards, gamma=0.99, entropy_coef=1e-2):\n",
        "    \"\"\"\n",
        "    Takes a sequence of states, actions and rewards produced by generate_session.\n",
        "    Updates agent's weights by following the policy gradient above.\n",
        "    Please use Adam optimizer with default parameters.\n",
        "    \"\"\"\n",
        "\n",
        "    # cast everything into torch tensors\n",
        "    states = torch.tensor(states, dtype=torch.float32)\n",
        "    actions = torch.tensor(actions, dtype=torch.int32)\n",
        "    cumulative_returns = np.array(get_cumulative_rewards(rewards, gamma))\n",
        "    cumulative_returns = torch.tensor(cumulative_returns, dtype=torch.float32)\n",
        "\n",
        "    # predict logits, probas and log-probas using an agent.\n",
        "    logits = model(states)\n",
        "    probs = nn.functional.softmax(logits, -1)\n",
        "    log_probs = nn.functional.log_softmax(logits, -1)\n",
        "\n",
        "    assert all(isinstance(v, torch.Tensor) for v in [logits, probs, log_probs]), \\\n",
        "        \"please use compute using torch tensors and don't use predict_probs function\"\n",
        "\n",
        "    # select log-probabilities for chosen actions, log pi(a_i|s_i)\n",
        "    log_probs_for_actions = torch.sum(\n",
        "        log_probs * to_one_hot(actions, env.action_space.n), dim=1)\n",
        "   \n",
        "    # Compute loss here. Don't forgen entropy regularization with `entropy_coef` \n",
        "    \n",
        "    entropy = torch.sum(probs*log_probs)\n",
        "    \n",
        "    \n",
        "    loss =-(torch.mean(log_probs_for_actions*cumulative_returns)+entropy_coef*entropy)\n",
        "\n",
        "    # Gradient descent step\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # technical: return session rewards to print them later\n",
        "    return np.sum(rewards)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3PI-OlDDI_z"
      },
      "source": [
        "### The actual training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnqs7m8wDI_z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 682
        },
        "outputId": "45c23a4d-45dc-4bc0-e468-d51e36e0ced8"
      },
      "source": [
        "for i in range(100):\n",
        "    rewards = [train_on_session(*generate_session(env)) for _ in range(100)]  # generate new sessions\n",
        "    \n",
        "    print(\"mean reward:%.3f\" % (np.mean(rewards)))\n",
        "    \n",
        "    if np.mean(rewards) > 300:\n",
        "        print(\"You Win!\")  # but you can train even further\n",
        "        break"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mean reward:9.360\n",
            "mean reward:9.330\n",
            "mean reward:9.330\n",
            "mean reward:9.440\n",
            "mean reward:9.270\n",
            "mean reward:9.410\n",
            "mean reward:9.480\n",
            "mean reward:9.340\n",
            "mean reward:9.480\n",
            "mean reward:9.340\n",
            "mean reward:9.310\n",
            "mean reward:9.330\n",
            "mean reward:9.370\n",
            "mean reward:9.460\n",
            "mean reward:9.190\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-4419e359ae85>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtrain_on_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mgenerate_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# generate new sessions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mean reward:%.3f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-4419e359ae85>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtrain_on_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mgenerate_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# generate new sessions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mean reward:%.3f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-75ae7ad3f896>\u001b[0m in \u001b[0;36mtrain_on_session\u001b[0;34m(states, actions, rewards, gamma, entropy_coef)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;31m# technical: return session rewards to print them later\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    116\u001b[0m                    \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m                    \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weight_decay'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m                    eps=group['eps'])\n\u001b[0m\u001b[1;32m    119\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/_functional.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_exp_avg_sqs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m             \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVDy06qADI_z"
      },
      "source": [
        "### Results & video"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EEHqa69TDI_z"
      },
      "source": [
        "# Record sessions\n",
        "\n",
        "import gym.wrappers\n",
        "\n",
        "with gym.wrappers.Monitor(gym.make(\"CartPole-v0\"), directory=\"videos\", force=True) as env_monitor:\n",
        "    sessions = [generate_session(env_monitor) for _ in range(100)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjEzk_cqDI_0"
      },
      "source": [
        "# Show video. This may not work in some setups. If it doesn't\n",
        "# work for you, you can download the videos and view them locally.\n",
        "\n",
        "from pathlib import Path\n",
        "from IPython.display import HTML\n",
        "\n",
        "video_names = sorted([s for s in Path('videos').iterdir() if s.suffix == '.mp4'])\n",
        "\n",
        "HTML(\"\"\"\n",
        "<video width=\"640\" height=\"480\" controls>\n",
        "  <source src=\"{}\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\".format(video_names[-1]))  # You can also try other indices"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVt9c2qjDI_0"
      },
      "source": [
        "from submit import submit_cartpole\n",
        "submit_cartpole(generate_session, 'your.email@example.com', 'YourAssignmentToken')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRww4DSnDI_0"
      },
      "source": [
        "That's all, thank you for your attention!\n",
        "\n",
        "Not having enough? There's an actor-critic waiting for you in the honor section. But make sure you've seen the videos first."
      ]
    }
  ]
}